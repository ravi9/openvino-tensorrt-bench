{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenVINO benchmarking with InceptionV3\n",
    "In this tutorial, we will use the Intel® Distribution of OpenVINO™ Toolkit to perform benchmarking on InceptionV3 model.\n",
    "\n",
    "This tutorial assumes that you have already downloaded and installed [Intel&reg; OpenVINO&trade;](https://software.intel.com/en-us/openvino-toolkit/choose-download) on your computer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use Intel® OpenVINO™, we need to do a few steps:\n",
    "\n",
    "1. Convert our Keras model to a Tensorflow model. \n",
    "1. Freeze the Tensorflow saved format model\n",
    "1. Use the OpenVINO Model Optimizer to convert the above freezed-model to the OpenVINO Intermediate Representation (IR) format\n",
    "1. Benchmark using the OpenVINO benchmark tool: `/opt/intel/openvino/deployment_tools/tools/benchmark_tool/benchmark_app.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:184: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3980: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\n",
      "96116736/96112376 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "model = keras.applications.inception_v3.InceptionV3(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = keras.backend.get_session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing the graph.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /home/ubuntu/models/inceptionv3/output/saved_model.pb\n",
      "TensorFlow protobuf version of model is saved in: /home/ubuntu/models/inceptionv3/output/\n",
      "Model input name =  input_1\n",
      "Model input shape =  (?, 299, 299, 3)\n",
      "Model output name =  predictions/Softmax\n",
      "Model output shape =  (?, 1000)\n"
     ]
    }
   ],
   "source": [
    "output_directory = \"/home/ubuntu/models/inceptionv3/output/\"\n",
    "print(\"Freezing the graph.\")\n",
    "keras.backend.set_learning_phase(0)\n",
    "\n",
    "signature = tf.saved_model.signature_def_utils.predict_signature_def(\n",
    "    inputs={'input': model.input}, outputs={'output': model.output})\n",
    "\n",
    "#If directory exists, delete it and let builder rebuild the TF model.\n",
    "if os.path.isdir(output_directory):\n",
    "    print (output_directory, \"exists already. Deleting the folder\")\n",
    "    shutil.rmtree(output_directory)\n",
    "\n",
    "builder = tf.saved_model.builder.SavedModelBuilder(output_directory)\n",
    "builder.add_meta_graph_and_variables(sess=sess,    \n",
    "                                     tags=[tf.saved_model.tag_constants.SERVING],    \n",
    "                                     signature_def_map={\n",
    "                                         tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:signature\n",
    "                                     })\n",
    "builder.save() \n",
    "print(\"TensorFlow protobuf version of model is saved in:\", output_directory)\n",
    "\n",
    "print(\"Model input name = \", model.input.op.name)\n",
    "print(\"Model input shape = \", model.input.shape)\n",
    "print(\"Model output name = \", model.output.op.name)\n",
    "print(\"Model output shape = \", model.output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:161: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/models/inceptionv3/output/variables/variables\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:233: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "INFO:tensorflow:Froze 378 variables.\n",
      "INFO:tensorflow:Converted 378 variables to const ops.\n",
      "TensorFlow Frozen model model is saved in: /home/ubuntu/models/inceptionv3/frozen_model/saved_model_frozen.pb\n"
     ]
    }
   ],
   "source": [
    "output_frozen_model_dir = \"/home/ubuntu/models/inceptionv3/frozen_model\"\n",
    "output_frozen_graph = output_frozen_model_dir+'/saved_model_frozen.pb'\n",
    "\n",
    "if not os.path.isdir(output_frozen_model_dir):\n",
    "    os.mkdir(output_frozen_model_dir)\n",
    "else:\n",
    "    print('Directory', output_frozen_model_dir, 'already exists. Deleting it and re-creating it')\n",
    "    shutil.rmtree(output_frozen_model_dir)\n",
    "    os.mkdir(output_frozen_model_dir)\n",
    "\n",
    "from tensorflow.python.tools.freeze_graph import freeze_graph\n",
    "\n",
    "_ = freeze_graph(input_graph=\"\",\n",
    "             input_saver=\"\",\n",
    "             input_binary=False,\n",
    "             input_checkpoint=\"\",\n",
    "             restore_op_name=\"save/restore_all\",\n",
    "             filename_tensor_name=\"save/Const:0\",\n",
    "             clear_devices=True,\n",
    "             initializer_nodes=\"\",\n",
    "             input_saved_model_dir=output_directory,\n",
    "             output_node_names=model.output.op.name,\n",
    "             output_graph=output_frozen_graph)\n",
    "\n",
    "print(\"TensorFlow Frozen model model is saved in:\", output_frozen_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \t/home/ubuntu/models/inceptionv3/frozen_model/saved_model_frozen.pb\n",
      "\t- Path for generated IR: \t/home/ubuntu/models/inceptionv3/IR_models/FP32\n",
      "\t- IR output name: \tsaved_model\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \tNot specified, inherited from the model\n",
      "\t- Output layers: \tNot specified, inherited from the model\n",
      "\t- Input shapes: \t[1,299,299,3]\n",
      "\t- Mean values: \tNot specified\n",
      "\t- Scale values: \tNot specified\n",
      "\t- Scale factor: \tNot specified\n",
      "\t- Precision of IR: \tFP32\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- Enable grouped convolutions fusing: \tTrue\n",
      "\t- Move mean values to preprocess section: \tFalse\n",
      "\t- Reverse input channels: \tFalse\n",
      "TensorFlow specific parameters:\n",
      "\t- Input model in text protobuf format: \tFalse\n",
      "\t- Path to model dump for TensorBoard: \tNone\n",
      "\t- List of shared libraries with TensorFlow custom layers implementation: \tNone\n",
      "\t- Update the configuration file with input/output node names: \tNone\n",
      "\t- Use configuration file used to generate the model with Object Detection API: \tNone\n",
      "\t- Operations to offload: \tNone\n",
      "\t- Patterns to offload: \tNone\n",
      "\t- Use the config file: \tNone\n",
      "Model Optimizer version: \t2020.1.0-61-gd349c3ba4a\n",
      "\n",
      "[ SUCCESS ] Generated IR version 10 model.\n",
      "[ SUCCESS ] XML file: /home/ubuntu/models/inceptionv3/IR_models/FP32/saved_model.xml\n",
      "[ SUCCESS ] BIN file: /home/ubuntu/models/inceptionv3/IR_models/FP32/saved_model.bin\n",
      "[ SUCCESS ] Total execution time: 35.85 seconds. \n",
      "[ SUCCESS ] Memory consumed: 889 MB. \n"
     ]
    }
   ],
   "source": [
    "output_frozen_model_dir = \"/home/ubuntu/models/inceptionv3/frozen_model\"\n",
    "output_frozen_graph = output_frozen_model_dir+'/saved_model_frozen.pb'\n",
    "\n",
    "if not os.path.exists(output_frozen_graph):\n",
    "    print(output_frozen_graph + ' doesn\\'t exist. Please make sure you have a trained keras to TF frozen model')\n",
    "\n",
    "!mo_tf.py \\\n",
    "      --input_model '/home/ubuntu/models/inceptionv3/frozen_model/saved_model_frozen.pb' \\\n",
    "      --input_shape=[1,299,299,3] \\\n",
    "      --data_type FP32  \\\n",
    "      --output_dir /home/ubuntu/models/inceptionv3/IR_models/FP32  \\\n",
    "      --model_name saved_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmark using the following command:\n",
    "```\n",
    "python3 /opt/intel/openvino/deployment_tools/tools/benchmark_tool/benchmark_app.py \\\n",
    "-m /home/ubuntu/models/inceptionv3/IR_models/FP32/saved_model.xml \\\n",
    "-nireq 1 -nstreams 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /opt/intel/openvino/deployment_tools/tools/benchmark_tool/benchmark_app.py \\\n",
    "-m /home/ubuntu/models/inceptionv3/IR_models/FP32/saved_model.xml \\\n",
    "-nireq 1 -nstreams 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Output\n",
    "```\n",
    "[Step 1/11] Parsing and validating input arguments\n",
    "[Step 2/11] Loading Inference Engine\n",
    "[ INFO ] InferenceEngine:\n",
    "         API version............. 2.1.37988\n",
    "[ INFO ] Device info\n",
    "         CPU\n",
    "         MKLDNNPlugin............ version 2.1\n",
    "         Build................... 37988\n",
    "\n",
    "[Step 3/11] Reading the Intermediate Representation network\n",
    "[Step 4/11] Resizing network to match image sizes and given batch\n",
    "[ INFO ] Network batch size: 1, precision: MIXED\n",
    "[Step 5/11] Configuring input of the model\n",
    "[Step 6/11] Setting device configuration\n",
    "[Step 7/11] Loading the model to the device\n",
    "[Step 8/11] Setting optimal runtime parameters\n",
    "[Step 9/11] Creating infer requests and filling input blobs with images\n",
    "[ INFO ] Network input 'input_1' precision U8, dimensions (NCHW): 1 3 299 299\n",
    "[ WARNING ] No input files were given: all inputs will be filled with random values!\n",
    "[ INFO ] Infer Request 0 filling\n",
    "[ INFO ] Fill input 'input_1' with random values (image is expected)\n",
    "[Step 10/11] Measuring performance (Start inference asyncronously, 1 inference requests using 1 streams for CPU, limits: 60000 ms duration)\n",
    "[Step 11/11] Dumping statistics report\n",
    "Count:      8279 iterations\n",
    "Duration:   60030.59 ms\n",
    "Latency:    6.93 ms\n",
    "Throughput: 137.91 FPS\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
