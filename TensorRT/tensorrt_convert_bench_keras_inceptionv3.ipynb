{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorRT benchmarking with InceptionV3\n",
    "In this tutorial, we will use the TensorRT to perform benchmarking on InceptionV3 model.\n",
    "\n",
    "This tutorial assumes that you running on [AWS Ubuntu DLAMI](https://aws.amazon.com/marketplace/pp/B07Y43P7X5). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are steps:\n",
    "\n",
    "1. Convert our Keras model to a Tensorflow model. \n",
    "1. Freeze the Tensorflow saved format model\n",
    "1. Convert the above freezed-model to the TensorRT formats: FP32 and FP16 (for V100)\n",
    "1. Benchmark with BZ=1, run the inference with BZ=1 for 1 min.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras as K\n",
    "import shutil, sys   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model inception_v3... \n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3980: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Model inception_v3... \")\n",
    "model = keras.applications.inception_v3.InceptionV3(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)\n",
    "print(\"Model loaded successfully\")\n",
    "\n",
    "sess = keras.backend.get_session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing the graph.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "/home/ubuntu/models/inceptionv3/output exists already. Deleting the folder\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /home/ubuntu/models/inceptionv3/output/saved_model.pb\n",
      "TensorFlow protobuf version of model is saved in: /home/ubuntu/models/inceptionv3/output\n",
      "Model input name =  input_1\n",
      "Model input shape =  (?, 299, 299, 3)\n",
      "Model output name =  predictions/Softmax\n",
      "Model output shape =  (?, 1000)\n"
     ]
    }
   ],
   "source": [
    "output_directory = \"/home/ubuntu/models/inceptionv3/output\"\n",
    "print(\"Freezing the graph.\")\n",
    "keras.backend.set_learning_phase(0)\n",
    "\n",
    "signature = tf.saved_model.signature_def_utils.predict_signature_def(\n",
    "    inputs={'input': model.input}, outputs={'output': model.output})\n",
    "\n",
    "\n",
    "if os.path.isdir(output_directory):\n",
    "    print (output_directory, \"exists already. Deleting the folder\")\n",
    "    shutil.rmtree(output_directory)\n",
    "\n",
    "builder = tf.saved_model.builder.SavedModelBuilder(output_directory)\n",
    "builder.add_meta_graph_and_variables(sess=sess,\n",
    "                                     tags=[tf.saved_model.tag_constants.SERVING],\n",
    "                                     signature_def_map={\n",
    "                                         tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:signature\n",
    "                                     })\n",
    "builder.save()\n",
    "print(\"TensorFlow protobuf version of model is saved in:\", output_directory)\n",
    "\n",
    "print(\"Model input name = \", model.input.op.name)\n",
    "print(\"Model input shape = \", model.input.shape)\n",
    "print(\"Model output name = \", model.output.op.name)\n",
    "print(\"Model output shape = \", model.output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting the graph to TensorRT.\n",
      "/home/ubuntu/models/inceptionv3/trt-output/ exists already. Deleting the folder\n",
      "INFO:tensorflow:Linked TensorRT version: (0, 0, 0)\n",
      "INFO:tensorflow:Loaded TensorRT version: (0, 0, 0)\n",
      "INFO:tensorflow:Running against TensorRT version 0.0.0\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/compiler/tensorrt/trt_convert.py:494: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/models/inceptionv3/output/variables/variables\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/compiler/tensorrt/trt_convert.py:517: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "INFO:tensorflow:Froze 378 variables.\n",
      "INFO:tensorflow:Converted 378 variables to const ops.\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /home/ubuntu/models/inceptionv3/trt-output/saved_model.pb\n",
      "Done. Converting the graph to TensorRT.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.compiler.tensorrt import trt_convert as trt\n",
    "\n",
    "print(\"Converting the graph to TensorRT.\")\n",
    "input_saved_model_dir = output_directory\n",
    "output_saved_model_dir = \"/home/ubuntu/models/inceptionv3/trt-output/\"\n",
    "\n",
    "#If directory exists, delete it and let builder rebuild the TF model.\n",
    "if os.path.isdir(output_saved_model_dir):\n",
    "    print (output_saved_model_dir, \"exists already. Deleting the folder\")\n",
    "    shutil.rmtree(output_saved_model_dir)\n",
    "    \n",
    "converter = trt.TrtGraphConverter(\n",
    "    input_saved_model_dir=input_saved_model_dir,\n",
    "    precision_mode=\"FP32\",\n",
    "    maximum_cached_engines=100)\n",
    "\n",
    "_ = converter.convert()\n",
    "_ = converter.save(output_saved_model_dir)\n",
    "\n",
    "\n",
    "print(\"Done. Converting the graph to TensorRT.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the variables. As the frozen graph donot have any variables and it raises error while serving.\n",
    "!cp -pra /home/ubuntu/models/inceptionv3/output/variables/ /home/ubuntu/models/inceptionv3/trt-output/  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/models/inceptionv3/trt-output/variables/variables\n",
      "\n",
      "Model: /home/ubuntu/models/inceptionv3/trt-output/, Input shape: (1, 299, 299, 3) , Output shape: (1, 1000) \n",
      "Completed Inference with one sample in 4.003 sec,\n"
     ]
    }
   ],
   "source": [
    "# Benchmark on one sample\n",
    "import time\n",
    "output_saved_model_dir = \"/home/ubuntu/models/inceptionv3/trt-output/\"\n",
    "output_tensor =  'predictions/Softmax:0'\n",
    "input_tensor = 'input_1:0'\n",
    "input_data = np.random.randint(0, 255, size=(1, 299, 299, 3))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # First load the SavedModel into the session\n",
    "    tf.saved_model.loader.load(\n",
    "        sess, [tf.saved_model.tag_constants.SERVING],\n",
    "       output_saved_model_dir)\n",
    "    start_time = time.time()\n",
    "    output = sess.run([output_tensor], feed_dict={input_tensor: input_data})\n",
    "    delta = (time.time() - start_time)\n",
    "\n",
    "\n",
    "print(\"\\nModel: {}, Input shape: {} , Output shape: {} \\nCompleted Inference with one sample in {:.3f} sec,\"\n",
    "      .format(output_saved_model_dir, input_data.shape, output[0].shape, delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Benchmark\n",
    "def benchmark_1min(output_saved_model_dir):\n",
    "    output_tensor =  'predictions/Softmax:0'\n",
    "    input_tensor = 'input_1:0'\n",
    "    input_data = np.random.randint(0, 255, size=(1, 299, 299, 3))\n",
    "\n",
    "    tf_config = tf.ConfigProto()\n",
    "    tf_config.gpu_options.allow_growth = True\n",
    "    tf_sess = tf.Session(config=tf_config)\n",
    "\n",
    "    # First load the SavedModel into the session\n",
    "    tf.saved_model.loader.load(\n",
    "            tf_sess, [tf.saved_model.tag_constants.SERVING],\n",
    "           output_saved_model_dir)\n",
    "\n",
    "    tf_sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    import time\n",
    "    times = []\n",
    "    # Run inference for 1 min.\n",
    "    end = time.time() + 60\n",
    "    print(time.strftime(\"%H:%M:%S\"))\n",
    "    print(\"Running inference for 1 min, with BZ=1\")\n",
    "    print(\"Model: {}, Input shape: {}  \"\n",
    "          .format(output_saved_model_dir, input_data.shape))\n",
    "    while time.time() < end:\n",
    "        start_time = time.time()\n",
    "        output = tf_sess.run([output_tensor], feed_dict={input_tensor: input_data})\n",
    "        delta = (time.time() - start_time)\n",
    "        times.append(delta)\n",
    "\n",
    "    mean_delta = np.array(times).mean()\n",
    "    fps = 1 / mean_delta\n",
    "    print('Output Shape: {}, \\naverage(sec):{:.3f} , average(msec):{:.2f} , fps:{:.2f}'\n",
    "          .format(output[0].shape, mean_delta, mean_delta*1000, fps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/models/inceptionv3/trt-output/variables/variables\n",
      "23:37:51\n",
      "Running inference for 1 min, with BZ=1\n",
      "Model: /home/ubuntu/models/inceptionv3/trt-output/, Input shape: (1, 299, 299, 3) , Output shape: (1, 1000) \n",
      "average(sec):0.027 , average(msec):26.62 , fps:37.57\n"
     ]
    }
   ],
   "source": [
    "benchmark_1min(\"/home/ubuntu/models/inceptionv3/trt-output/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert and benchmark FP16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting the graph to TensorRT.\n",
      "/home/ubuntu/models/inceptionv3/trt-output-fp16/ exists already. Deleting the folder\n",
      "INFO:tensorflow:Linked TensorRT version: (0, 0, 0)\n",
      "INFO:tensorflow:Loaded TensorRT version: (0, 0, 0)\n",
      "INFO:tensorflow:Running against TensorRT version 0.0.0\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/models/inceptionv3/output/variables/variables\n",
      "INFO:tensorflow:Froze 378 variables.\n",
      "INFO:tensorflow:Converted 378 variables to const ops.\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /home/ubuntu/models/inceptionv3/trt-output-fp16/saved_model.pb\n",
      "Done. Converting the graph to TensorRT-FP16.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.compiler.tensorrt import trt_convert as trt\n",
    "\n",
    "print(\"Converting the graph to TensorRT.\")\n",
    "input_saved_model_dir = \"/home/ubuntu/models/inceptionv3/output\"\n",
    "output_saved_model_dir = \"/home/ubuntu/models/inceptionv3/trt-output-fp16/\"\n",
    "\n",
    "#If directory exists, delete it and let builder rebuild the TF model.\n",
    "if os.path.isdir(output_saved_model_dir):\n",
    "    print (output_saved_model_dir, \"exists already. Deleting the folder\")\n",
    "    shutil.rmtree(output_saved_model_dir)\n",
    "    \n",
    "converter = trt.TrtGraphConverter(\n",
    "    input_saved_model_dir=input_saved_model_dir,\n",
    "    precision_mode=\"FP16\",\n",
    "    maximum_cached_engines=100)\n",
    "\n",
    "_ = converter.convert()\n",
    "_ = converter.save(output_saved_model_dir)\n",
    "\n",
    "\n",
    "print(\"Done. Converting the graph to TensorRT-FP16.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the variables. As the frozen graph donot have any variables and it raises error while serving.\n",
    "!cp -pra /home/ubuntu/models/inceptionv3/output/variables/ /home/ubuntu/models/inceptionv3/trt-output-fp16/  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/models/inceptionv3/trt-output-fp16/variables/variables\n",
      "\n",
      "Model: /home/ubuntu/models/inceptionv3/trt-output-fp16/, Input shape: (1, 299, 299, 3) , Output shape: (1, 1000) \n",
      "Completed Inference with one sample in 2.768 sec,\n"
     ]
    }
   ],
   "source": [
    "# Benchmark on one sample\n",
    "import time\n",
    "output_saved_model_dir = \"/home/ubuntu/models/inceptionv3/trt-output-fp16/\"\n",
    "output_tensor =  'predictions/Softmax:0'\n",
    "input_tensor = 'input_1:0'\n",
    "input_data = np.random.randint(0, 255, size=(1, 299, 299, 3))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # First load the SavedModel into the session\n",
    "    tf.saved_model.loader.load(\n",
    "        sess, [tf.saved_model.tag_constants.SERVING],\n",
    "       output_saved_model_dir)\n",
    "    start_time = time.time()\n",
    "    output = sess.run([output_tensor], feed_dict={input_tensor: input_data})\n",
    "    delta = (time.time() - start_time)\n",
    "\n",
    "\n",
    "print(\"\\nModel: {}, Input shape: {} , Output shape: {} \\nCompleted Inference with one sample in {:.3f} sec,\"\n",
    "      .format(output_saved_model_dir, input_data.shape, output[0].shape, delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/models/inceptionv3/trt-output-fp16/variables/variables\n",
      "23:40:47\n",
      "Running inference for 1 min, with BZ=1\n",
      "Model: /home/ubuntu/models/inceptionv3/trt-output-fp16/, Input shape: (1, 299, 299, 3) , Output shape: (1, 1000) \n",
      "average(sec):0.027 , average(msec):26.77 , fps:37.35\n"
     ]
    }
   ],
   "source": [
    "## Benchmark\n",
    "benchmark_1min(\"/home/ubuntu/models/inceptionv3/trt-output-fp16/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Mar 27 00:11:47 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.87.01    Driver Version: 418.87.01    CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla M60           On   | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   30C    P8    23W / 150W |      0MiB /  7618MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture:        x86_64\n",
      "CPU op-mode(s):      32-bit, 64-bit\n",
      "Byte Order:          Little Endian\n",
      "CPU(s):              16\n",
      "On-line CPU(s) list: 0-15\n",
      "Thread(s) per core:  2\n",
      "Core(s) per socket:  8\n",
      "Socket(s):           1\n",
      "NUMA node(s):        1\n",
      "Vendor ID:           GenuineIntel\n",
      "CPU family:          6\n",
      "Model:               79\n",
      "Model name:          Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz\n",
      "Stepping:            1\n",
      "CPU MHz:             2008.228\n",
      "CPU max MHz:         3000.0000\n",
      "CPU min MHz:         1200.0000\n",
      "BogoMIPS:            4600.08\n",
      "Hypervisor vendor:   Xen\n",
      "Virtualization type: full\n",
      "L1d cache:           32K\n",
      "L1i cache:           32K\n",
      "L2 cache:            256K\n",
      "L3 cache:            46080K\n",
      "NUMA node0 CPU(s):   0-15\n",
      "Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single pti fsgsbase bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx xsaveopt\n"
     ]
    }
   ],
   "source": [
    "!lscpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
